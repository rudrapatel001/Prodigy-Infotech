# Prodigy Infotech Data Science Internship

Welcome to my GitHub repository for the Data Science Internship at Prodigy Infotech! This repository contains all the tasks and projects assigned during the internship, along with their implementations and my learning progress.

## Table of Contents

- [Overview](#overview)
- [Internship Goals](#internship-goals)
- [Projects and Tasks](#projects-and-tasks)
- [Technologies Used](#technologies-used)
- [Setup and Installation](#setup-and-installation)
- [How to Use](#how-to-use)
- [Contact](#contact)

## Overview

This repository serves as a comprehensive collection of the work I have done during my data science internship at Prodigy Infotech. The internship spans various topics and projects, focusing on developing practical data science skills.

## Internship Goals

The main objectives of this internship include:

- Gaining hands-on experience with data analysis and machine learning techniques.
- Implementing real-world data science projects using various tools and technologies.
- Enhancing problem-solving skills and understanding of the data science workflow.
- Collaborating with team members and learning best practices in data science.

## Projects and Tasks

Below is a list of the tasks and projects I've worked on during the internship. Each task has its dedicated folder containing the code, data, and related documentation.

1. **Task 1: Visualizing Population Distribution Using Bar Charts and Histograms**
   - Description: In this task, the objective was to create visual representations of population distribution across different years using bar charts and histograms. The task involved handling and cleaning a dataset containing population data from various countries over several decades. The visualizations aimed to provide insights into the distribution of populations across different years, allowing for an analysis of trends and patterns over time.  .
   - Tools Used: **Python**: The primary programming language used for data manipulation and visualization.
                 **Pandas**: For data loading, cleaning, and manipulation.
                 **Matplotlib**: For creating static visualizations, such as bar charts and histograms.
                 **Seaborn**: For enhanced and aesthetically pleasing visualizations, including histograms with KDE (Kernel Density Estimate) plots.
                 **Jupyter Notebook**: As the development environment for writing and executing the code.
   - Outcome: The task resulted in the successful generation of bar charts and histograms for each year in the dataset, showcasing the distribution of populations across different countries. The visualizations revealed patterns such as the growth in population over time and differences in distribution across countries. The process also involved cleaning the data by handling missing values, ensuring the accuracy of the visualizations. These visualizations can now serve as a basis for further analysis and reporting on population trends..

2. **Task 2: Data Cleaning and Exploratory Data Analysis on the Titanic Dataset**
   - Description: In this task, the goal was to perform data cleaning and exploratory data analysis (EDA) on the Titanic dataset, a well-known dataset from Kaggle. The dataset includes information about the passengers on the Titanic, such as their age, gender, ticket class, and survival status. The task involved cleaning the dataset by handling missing values, correcting data types, and removing or imputing outliers. After cleaning the data, EDA was conducted to explore relationships between variables, identify patterns, and uncover insights about the factors that influenced passenger survival.
   - Tools Used: **Python**: The programming language used for data cleaning and analysis.
**Pandas**: For data manipulation, cleaning, and exploratory analysis.
**NumPy**: For numerical operations and handling missing data.
**Matplotlib**: For creating basic visualizations to support EDA.
**Seaborn**: For advanced visualizations to explore relationships between variables, including pair plots, heatmaps, and bar charts.
**Jupyter Notebook**: The environment used for writing and running the analysis code.
   - Outcome: The task successfully resulted in a clean and well-structured Titanic dataset, ready for further analysis or modeling. The exploratory data analysis provided key insights, such as the impact of passenger class, gender, and age on survival rates. For instance, the analysis revealed that women and children had higher survival rates, and that passengers in first class were more likely to survive compared to those in lower classes. The visualizations created during EDA helped to highlight these relationships and trends, making the data more understandable and actionable.

3. **Task 3: Building a Decision Tree Classifier for Customer Purchase Prediction**
   - Description: In this task, the objective was to build a decision tree classifier to predict whether a customer would purchase a product or service based on their demographic and behavioral data. The task utilized the Bank Marketing dataset from the UCI Machine Learning Repository, which includes customer data such as age, job, marital status, education, and previous marketing campaign outcomes. The goal was to develop a predictive model that can help in identifying potential customers who are more likely to purchase a product, thereby optimizing marketing efforts.
   - Tools Used: **Python**: The programming language used for data processing, modeling, and evaluation.
**Pandas**: For data loading, preprocessing, and manipulation.
**Scikit-learn**: For implementing the decision tree classifier, performing data splitting, and evaluating the model's performance.
**Matplotlib & Seaborn**: For visualizing the decision tree, feature importance, and other relevant insights.
**Jupyter Notebook**: The development environment used for the entire workflow, including code execution, visualization, and documentation.
   - Outcome: The task successfully resulted in a trained decision tree classifier capable of predicting customer purchases with a good level of accuracy. The model was evaluated using metrics such as accuracy, precision, recall, and the F1-score, ensuring its effectiveness in making predictions. The analysis also included a visualization of the decision tree, providing insight into how different demographic and behavioral factors influence purchasing decisions. Additionally, the importance of each feature in the decision-making process was assessed, helping to identify the most influential variables. The model can be used to target specific customer segments, improving the efficiency of marketing campaigns.



## Technologies Used

Throughout this internship, I have worked with various tools and technologies, including but not limited to:

- **Programming Languages:** Python, SQL
- **Libraries and Frameworks:** Pandas, NumPy, Scikit-learn, Matplotlib, Seaborn, TensorFlow, etc.
- **Data Visualization:** Power BI, Tableau
- **Version Control:** Git, GitHub
- **Database Management:** MySQL
- **Others:** Jupyter Notebook, Google Colab

## Setup and Installation

To replicate or explore the work done in this repository, follow these steps:

1. Clone the repository:
   ```bash
   git clone https://github.com/your-username/your-repo-name.git
   ```

2. Navigate to the project directory:
   ```bash
   cd your-repo-name
   ```

3. Install the required dependencies:
   ```bash
   pip install -r requirements.txt
   ```

   _(Ensure you have Python installed and a virtual environment activated)_

4. Open the project in your preferred IDE or text editor.

## How to Use

Each folder contains detailed instructions and documentation related to the respective task or project. You can run the Jupyter notebooks or Python scripts directly to see the implementation in action.

## Contact

If you have any questions or would like to connect, feel free to reach out to me:

- **Name:** Rudra Patel
- **Email:** rudra7042004@gmail.com
- **LinkedIn:** https://www.linkedin.com/in/rudra-patel-515441242/

Thank you for visiting my repository!

---
